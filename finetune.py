import numpy as np
import pandas as pd
import torch
from torch.optim import Adam
from torch.utils.data import DataLoader
from tqdm import tqdm

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from tabpfn import TabPFNRegressor
from tabpfn.finetune_utils import clone_model_for_evaluation
from tabpfn.utils import meta_dataset_collator
import matplotlib.pyplot as plt


# Branin å‡½æ•°ï¼ˆæ ‡å‡†å®šä¹‰ï¼ŒåŸŸï¼šx1 âˆˆ [-5, 10], x2 âˆˆ [0, 15]ï¼‰
def branin(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
    a = 1.0
    b = 5.1 / (4.0 * np.pi**2)
    c = 5.0 / np.pi
    r = 6.0
    s = 10.0
    t = 1.0 / (8.0 * np.pi)

    return (
        a * (x2 - b * x1**2 + c * x1 - r) ** 2
        + s * (1 - t) * np.cos(x1)
        + s
    ).astype(np.float32)


def prepare_data(
    config: dict,
) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    å‡†å¤‡ Branin å¾®è°ƒæ•°æ®ã€‚

    è¿”å›å››ä¸ª ndarrayï¼š
    - X_ctx_pool:  ç”¨äºåç»­éšæœºé‡‡æ ·ä¸Šä¸‹æ–‡ç‚¹çš„â€œæ± â€ï¼ˆæ¥è‡ª branin_dataset.csvï¼‰
    - X_test_grid: å…¨å±€ 30x30 ç½‘æ ¼çš„è¾“å…¥ç‚¹
    - y_ctx_pool:  X_ctx_pool å¯¹åº”çš„ y
    - y_test_grid: X_test_grid å¯¹åº”çš„ yï¼ˆç”¨è§£æ Branin å‡½æ•°è®¡ç®—ï¼‰

    ä¸Šä¸‹æ–‡é•¿åº¦ 2â€“20 çš„å˜åŒ– **ä¸åœ¨è¿™é‡Œå®ç°**ï¼Œ
    è€Œæ˜¯åç»­åœ¨é‡‡æ ·ä»»åŠ¡æ—¶ï¼Œä» X_ctx_pool / y_ctx_pool é‡ŒæŒ‰éœ€æŠ½å–ã€‚
    """
    print("--- 1. Data Preparation (Branin) ---")

    # ========== 1) è¯»å– branin_dataset.csv ==========
    data_path = config.get("data_path", "./data/branin_dataset.csv")
    df = pd.read_csv(data_path)

    # å‡å®šåˆ—åä¸º: x1, x2, y
    X_all = df[["x1", "x2"]].values.astype(np.float32)
    y_all = df["y"].values.astype(np.float32)

    # å…è®¸é€šè¿‡ num_samples_to_use ä¸‹é‡‡æ ·
    rng = np.random.default_rng(config.get("random_seed", 42))
    num_samples_to_use = min(config.get("num_samples_to_use", len(y_all)), len(y_all))
    indices = rng.choice(len(y_all), size=num_samples_to_use, replace=False)

    X_ctx_pool = X_all[indices]
    y_ctx_pool = y_all[indices]

    print(f"Context pool: {X_ctx_pool.shape[0]} samples")
    print(f"  X_ctx_pool shape: {X_ctx_pool.shape}")  # (N_ctx_pool, 2)
    print(f"  y_ctx_pool shape: {y_ctx_pool.shape}")  # (N_ctx_pool,)

    # ========== 2) æ„é€ å…¨å±€ 30x30 ç½‘æ ¼ä½œä¸º test ==========
    x1_min, x1_max = config.get("x1_range", (-5.0, 10.0))
    x2_min, x2_max = config.get("x2_range", (0.0, 15.0))
    grid_size = config.get("grid_size", 30)

    x1_lin = np.linspace(x1_min, x1_max, grid_size, dtype=np.float32)
    x2_lin = np.linspace(x2_min, x2_max, grid_size, dtype=np.float32)
    X1, X2 = np.meshgrid(x1_lin, x2_lin)  # æ¯ä¸ªéƒ½æ˜¯ (grid_size, grid_size)

    # å±•å¹³æˆ (grid_size^2, 2)
    X_test_grid = np.stack([X1.ravel(), X2.ravel()], axis=1).astype(np.float32)
    y_test_grid = branin(X_test_grid[:, 0], X_test_grid[:, 1])

    print(f"Global test grid: {X_test_grid.shape[0]} points")
    print(f"  X_test_grid shape: {X_test_grid.shape}")  # (grid_size^2, 2)
    print(f"  y_test_grid shape: {y_test_grid.shape}")  # (grid_size^2,)

    # ç®€å•å±•ç¤ºå‰å‡ è¡Œæ•°æ®ï¼Œå¸®åŠ©ç›´è§‚æŸ¥çœ‹
    print("\n[Sample of context pool X, y]:")
    print(np.concatenate(
        [X_ctx_pool[:5], y_ctx_pool[:5, None]],
        axis=1
    ))

    print("\n[Sample of test grid X, y]:")
    print(np.concatenate(
        [X_test_grid[:5], y_test_grid[:5, None]],
        axis=1
    ))

    print("---------------------------\n")

    return X_ctx_pool, X_test_grid, y_ctx_pool, y_test_grid


import numpy as np
from torch.utils.data import Dataset, DataLoader


class BraninMetaDataset(Dataset):
    """
    æ¯ä¸ª __getitem__ è¿”å›ä¸€ä¸ªâ€œæºä»»åŠ¡â€ï¼š
      - X_ctx:  (n_ctx, 2)   éšæœºé‡‡æ ·çš„ä¸Šä¸‹æ–‡ç‚¹ï¼Œn_ctx âˆˆ [min_ctx, max_ctx]
      - y_ctx:  (n_ctx,)
      - X_test: (n_test, 2)  å…¨å±€ç½‘æ ¼ç‚¹ï¼ˆå›ºå®šï¼‰
      - y_test: (n_test,)
    """
    def __init__(
        self,
        X_ctx_pool: np.ndarray,
        y_ctx_pool: np.ndarray,
        X_test_grid: np.ndarray,
        y_test_grid: np.ndarray,
        num_tasks: int = 1000,
        min_ctx: int = 2,
        max_ctx: int = 20,
        random_seed: int = 42,
    ):
        assert X_ctx_pool.shape[0] == y_ctx_pool.shape[0], "ctx pool size mismatch"
        assert X_test_grid.shape[0] == y_test_grid.shape[0], "test grid size mismatch"
        assert min_ctx >= 1, "min_ctx must be >= 1"
        assert max_ctx >= min_ctx, "max_ctx must be >= min_ctx"

        self.X_ctx_pool = X_ctx_pool
        self.y_ctx_pool = y_ctx_pool
        self.X_test_grid = X_test_grid
        self.y_test_grid = y_test_grid

        self.num_tasks = num_tasks
        self.min_ctx = min_ctx
        self.max_ctx = max_ctx

        self.rng = np.random.default_rng(random_seed)

    def __len__(self) -> int:
        # æ•°æ®é›†ä¸­ä¸€å…±å¤šå°‘ä¸ªâ€œæºä»»åŠ¡â€
        return self.num_tasks

    def __getitem__(self, idx: int):
        # 1) éšæœºå†³å®šè¿™ä¸ªä»»åŠ¡çš„ä¸Šä¸‹æ–‡é•¿åº¦ n_ctx âˆˆ [min_ctx, max_ctx]
        n_pool = self.X_ctx_pool.shape[0]
        n_ctx = int(self.rng.integers(self.min_ctx, self.max_ctx + 1))

        # 2) åœ¨ context pool ä¸­æ— æ”¾å›éšæœºé‡‡æ · n_ctx ä¸ªç‚¹
        indices = self.rng.choice(n_pool, size=n_ctx, replace=False)
        X_ctx = self.X_ctx_pool[indices]   # (n_ctx, 2)
        y_ctx = self.y_ctx_pool[indices]   # (n_ctx,)

        # 3) test ç›´æ¥ç”¨å›ºå®šçš„å…¨å±€ç½‘æ ¼
        X_test = self.X_test_grid         # (n_test, 2)
        y_test = self.y_test_grid         # (n_test,)

        return X_ctx, y_ctx, X_test, y_test

def demo_meta_dataset(
    X_ctx_pool,
    X_test_grid,
    y_ctx_pool,
    y_test_grid,
):
    # åˆ›å»ºä¸€ä¸ªåŒ…å« 5 ä¸ªä»»åŠ¡çš„å°æ•°æ®é›†å…ˆè¯•è¯•
    meta_ds = BraninMetaDataset(
        X_ctx_pool=X_ctx_pool,
        y_ctx_pool=y_ctx_pool,
        X_test_grid=X_test_grid,
        y_test_grid=y_test_grid,
        num_tasks=5,      # å…ˆæ¥ 5 ä¸ªä»»åŠ¡çœ‹ä¸€çœ¼
        min_ctx=2,
        max_ctx=20,
        random_seed=123,  # ä¸ºäº†å¯å¤ç°
    )

    # ç®€å• DataLoaderï¼Œbatch_size=1 è¡¨ç¤ºä¸€æ¬¡å–ä¸€ä¸ªâ€œä»»åŠ¡â€
    loader = DataLoader(meta_ds, batch_size=1, shuffle=False)

    print("--- 2. Inspect a few sampled tasks ---")
    for batch_idx, batch in enumerate(loader):
        X_ctx, y_ctx, X_test, y_test = batch  # æ³¨æ„è¿™æ˜¯åŠ äº† batch ç»´åº¦çš„

        # batch_size=1ï¼Œæ‰€ä»¥å¯ä»¥å»æ‰ç¬¬ä¸€ç»´æ›´ç›´è§‚
        X_ctx = X_ctx[0].numpy()
        y_ctx = y_ctx[0].numpy()
        X_test = X_test[0].numpy()
        y_test = y_test[0].numpy()

        print(f"\nTask {batch_idx}:")
        print(f"  X_ctx shape:  {X_ctx.shape}")   # (n_ctx, 2)
        print(f"  y_ctx shape:  {y_ctx.shape}")   # (n_ctx,)")
        print(f"  X_test shape: {X_test.shape}")  # (n_test, 2)
        print(f"  y_test shape: {y_test.shape}")  # (n_test,)")

        # å±•ç¤ºå‰å‡ ä¸ªä¸Šä¸‹æ–‡ç‚¹å’Œç½‘æ ¼ç‚¹ï¼Œç›´è§‚æ„Ÿå—ä¸€ä¸‹
        print("  Sample context points (x1, x2, y):")
        print(np.concatenate([X_ctx[:3], y_ctx[:3, None]], axis=1))

        print("  Sample test grid points (x1, x2, y):")
        print(np.concatenate([X_test[:3], y_test[:3, None]], axis=1))

        if batch_idx >= 2:
            # åªçœ‹å‰ä¸‰ä¸ªä»»åŠ¡ï¼Œå¤Ÿæ£€æŸ¥ç»“æ„äº†
            break

    print("---------------------------\n")

def setup_regressor(config: dict) -> tuple[TabPFNRegressor, dict]:
    """åˆå§‹åŒ– TabPFNRegressorï¼Œå¹¶è¿”å›æ¨¡å‹æœ¬èº«å’Œå®ƒçš„é…ç½®å­—å…¸ã€‚"""
    print("--- 3. Model Setup (TabPFNRegressor) ---")

    regressor_config = {
        "ignore_pretraining_limits": True,
        "device": config["device"],
        "n_estimators": 1,  # åªç”¨å•æ¨¡å‹ï¼Œæ–¹ä¾¿å¾®è°ƒ
        "random_state": config["random_seed"],
        "inference_precision": torch.float32,
    }

    regressor = TabPFNRegressor(
        **regressor_config,
        fit_mode="batched",
        differentiable_input=False,
    )

    print(f"Using device: {config['device']}")
    print("---------------------------\n")
    return regressor, regressor_config

import numpy as np


def make_branin_splitter(
    X_test_grid: np.ndarray,
    y_test_grid: np.ndarray,
    config: dict,
):
    """
    è¿”å›ä¸€ä¸ªè‡ªå®šä¹‰ splitterï¼Œç”¨åœ¨ TabPFNRegressor.get_preprocessed_datasets é‡Œã€‚

    ä½œç”¨ï¼š
      - è¾“å…¥: (X_all, y_all) â€”â€” å®é™…ä¸Šå°±æ˜¯æˆ‘ä»¬çš„ context pool
      - è¾“å‡º: (X_ctx, X_test, y_ctx, y_test)
        å…¶ä¸­:
          * X_ctx, y_ctx: ä» X_all, y_all ä¸­éšæœºæŠ½å– kâˆˆ[min_context, max_context] ä¸ªç‚¹
          * X_test, y_test: å›ºå®šä¸ºå…¨å±€ Branin ç½‘æ ¼
    """
    min_c = config["min_context"]
    max_c = config["max_context"]
    rng = np.random.default_rng(config["random_seed"])

    def splitter(X_all: np.ndarray, y_all: np.ndarray):
        n = X_all.shape[0]
        ctx_size = rng.integers(min_c, max_c + 1)
        indices = rng.choice(np.arange(n), size=ctx_size, replace=False)

        X_ctx = X_all[indices]
        y_ctx = y_all[indices]

        # test éƒ¨åˆ†ç›´æ¥ç”¨å…¨å±€ç½‘æ ¼
        X_test = X_test_grid
        y_test = y_test_grid

        return X_ctx, X_test, y_ctx, y_test

    return splitter

def create_finetuning_dataloader(
    regressor: TabPFNRegressor,
    X_ctx_pool: np.ndarray,
    y_ctx_pool: np.ndarray,
    X_test_grid: np.ndarray,
    y_test_grid: np.ndarray,
    config: dict,
) -> DataLoader:
    print("--- 4. Build finetuning datasets & dataloader ---")

    splitter = make_branin_splitter(X_test_grid, y_test_grid, config)

    # è¿™é‡Œçš„ max_data_size å¯¹æˆ‘ä»¬è‡ªå®šä¹‰çš„ splitter å®é™…å½±å“ä¸å¤§ï¼Œ
    # å®ƒä¸»è¦ä½œç”¨åœ¨ TabPFN å†…éƒ¨çš„é¢„å¤„ç†ï¼›è®¾å¾—ç•¥å¤§ä¸€ç‚¹å³å¯ã€‚
    max_data_size = config["finetuning"]["max_data_size"]

    training_datasets = regressor.get_preprocessed_datasets(
        X_ctx_pool,
        y_ctx_pool,
        splitter,
        max_data_size=max_data_size,
    )

    print(f"Number of meta-datasets from get_preprocessed_datasets: {len(training_datasets)}")

    finetuning_dataloader = DataLoader(
        training_datasets,
        batch_size=config["finetuning"]["meta_batch_size"],
        collate_fn=meta_dataset_collator,
    )

    # â€”â€” æ‰“å°ä¸€ä¸ª batchï¼Œç¡®è®¤å½¢çŠ¶æ˜¯å¦åˆç† â€”â€”
    first_batch = next(iter(finetuning_dataloader))

    (
        X_trains_preprocessed,
        X_tests_preprocessed,
        y_trains_znorm,
        y_test_znorm,
        cat_ixs,
        confs,
        raw_space_bardist_,
        znorm_space_bardist_,
        _,
        _y_test_raw,
    ) = first_batch

    print("Inspect one preprocessed task (after collate, meta_batch_size=1):")
    print("  X_trains_preprocessed[0].shape:", X_trains_preprocessed[0].shape)
    print("  X_tests_preprocessed[0].shape :", X_tests_preprocessed[0].shape)
    print("  y_trains_znorm[0].shape       :", y_trains_znorm[0].shape)
    print("  y_test_znorm[0].shape         :", y_test_znorm[0].shape)
    print("---------------------------\n")

    return finetuning_dataloader

def evaluate_regressor_on_branin_grid(
    regressor: TabPFNRegressor,
    eval_config: dict,
    X_ctx_pool: np.ndarray,
    y_ctx_pool: np.ndarray,
    X_test_grid: np.ndarray,
    y_test_grid: np.ndarray,
    config: dict,
) -> tuple[float, float, float]:
    """åœ¨å…¨å±€ Branin ç½‘æ ¼ä¸Šè¯„ä¼°å½“å‰ï¼ˆå¾®è°ƒåçš„ï¼‰TabPFNã€‚"""
    eval_regressor = clone_model_for_evaluation(regressor, eval_config, TabPFNRegressor)

    rng = np.random.default_rng(config["random_seed"])
    n_ctx_eval = config.get("eval_context_size", config["max_context"])

    idx = rng.choice(np.arange(X_ctx_pool.shape[0]), size=n_ctx_eval, replace=False)
    X_ctx_eval = X_ctx_pool[idx]
    y_ctx_eval = y_ctx_pool[idx]

    # è¿™é‡Œçš„ fit æ˜¯â€œç»™ä¸Šä¸‹æ–‡â€ï¼Œä¸ä¼šå†åšæ¢¯åº¦æ›´æ–°ï¼Œåªæ˜¯ç”¨äº ICL æ¨ç†
    eval_regressor.fit(X_ctx_eval, y_ctx_eval)

    preds = eval_regressor.predict(X_test_grid)

    mse = mean_squared_error(y_test_grid, preds)
    mae = mean_absolute_error(y_test_grid, preds)
    r2 = r2_score(y_test_grid, preds)

    return mse, mae, r2

def run_finetuning(
    regressor: TabPFNRegressor,
    regressor_config: dict,
    finetuning_dataloader: DataLoader,
    X_ctx_pool: np.ndarray,
    y_ctx_pool: np.ndarray,
    X_test_grid: np.ndarray,
    y_test_grid: np.ndarray,
    config: dict,
) -> None:
    # æ³¨æ„ï¼šå¿…é¡»å…ˆè°ƒç”¨ get_preprocessed_datasetsï¼Œå†æ‹¿ models_[0] åˆå§‹åŒ–ä¼˜åŒ–å™¨
    if len(regressor.models_) > 1:
        raise ValueError(
            f"Your TabPFNRegressor uses multiple models ({len(regressor.models_)}). "
            "Finetuning is only supported for a single model."
        )

    model = regressor.models_[0]
    optimizer = Adam(model.parameters(), lr=config["finetuning"]["learning_rate"])

    print(
        f"--- Optimizer Initialized: Adam, LR: {config['finetuning']['learning_rate']} ---\n"
    )

    # è¯„ä¼°æ—¶çš„é…ç½®
    eval_config = {
        **regressor_config,
        "inference_config": {
            # è¿™é‡Œç»™ä¸ªä¸Šç•Œï¼ŒBranin ä¸Šæˆ‘ä»¬å…¶å®åªç”¨åˆ° <= max_context
            "SUBSAMPLE_SAMPLES": config["n_inference_context_samples"],
        },
    }

    print("--- 5. Starting Finetuning & Evaluation (Branin) ---")

    num_epochs = config["finetuning"]["epochs"]

    for epoch in range(num_epochs + 1):
        # 1) å…ˆåœ¨ Branin å…¨å±€ç½‘æ ¼ä¸Šè¯„ä¼°å½“å‰æ¨¡å‹
        mse, mae, r2 = evaluate_regressor_on_branin_grid(
            regressor,
            eval_config,
            X_ctx_pool,
            y_ctx_pool,
            X_test_grid,
            y_test_grid,
            config,
        )
        status = "Initial" if epoch == 0 else f"Epoch {epoch}"
        print(
            f"ğŸ“Š {status} Evaluation on Branin grid | "
            f"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}"
        )

        # 2) ä» epoch=1 å¼€å§‹åšå¾®è°ƒ
        if epoch == 0:
            print("---------------------------")
            continue

        progress_bar = tqdm(finetuning_dataloader, desc=f"Finetuning Epoch {epoch}")
        for data_batch in progress_bar:
            optimizer.zero_grad()

            (
                X_trains_preprocessed,
                X_tests_preprocessed,
                y_trains_znorm,
                y_test_znorm,
                cat_ixs,
                confs,
                raw_space_bardist_,
                znorm_space_bardist_,
                _,
                _y_test_raw,
            ) = data_batch

            # å–å‡º batch ä¸­çš„ç¬¬ 0 ä¸ªä»»åŠ¡ï¼ˆå› ä¸º meta_batch_size=1ï¼‰
            regressor.raw_space_bardist_ = raw_space_bardist_[0]
            regressor.znorm_space_bardist_ = znorm_space_bardist_[0]

            regressor.fit_from_preprocessed(
                X_trains_preprocessed,
                y_trains_znorm,
                cat_ixs,
                confs,
            )

            logits, _, _ = regressor.forward(X_tests_preprocessed)

            # å›å½’ä»»åŠ¡çš„ loss function å·²ç»åŒ…å«åœ¨ znorm_space_bardist_ é‡Œ
            loss_fn = znorm_space_bardist_[0]
            y_target = y_test_znorm

            loss = loss_fn(logits, y_target.to(config["device"])).mean()
            loss.backward()
            optimizer.step()

            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

        print("---------------------------")

    # # åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ å¯¼å…¥  
    from tabpfn.model_loading import save_tabpfn_model  
    
    # ä¿®æ”¹mainå‡½æ•°æœ«å°¾çš„ä¿å­˜éƒ¨åˆ†  
    save_path = "./model/finetuned_tabpfn_branin.ckpt"  
    save_tabpfn_model(regressor, save_path)  
    print(f"Saved fine-tuned model weights to: {save_path}")


    print("--- âœ… Finetuning Finished ---")


def main() -> None:
    # â€”â€” å…¨å±€é…ç½® â€”â€” 
    config = {
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "random_seed": 42,

        # Branin æ•°æ®é…ç½®
        "num_context_samples": 10_000,  # context pool å¤§å°
        "grid_size": 20,                # 20Ã—20 ç½‘æ ¼ -> 400 ä¸ª test ç‚¹

        # å˜é•¿ä¸Šä¸‹æ–‡
        "min_context": 2,
        "max_context": 20,

        # è¯„ä¼°æ—¶ä½¿ç”¨å¤šå°‘ä¸ªä¸Šä¸‹æ–‡ç‚¹
        "eval_context_size": 20,

        # è¯„ä¼°æ—¶ TabPFN æ¨ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡æ•°é‡ä¸Šç•Œ
        "n_inference_context_samples": 20,
    }

    # å¾®è°ƒè¶…å‚æ•°
    config["finetuning"] = {
        "epochs": 10,              # å…ˆæ¥å‡ ä¸ª epoch è¯•è¯•
        "learning_rate": 1.5e-6,  # å®˜æ–¹æ¨èçš„å°å­¦ä¹ ç‡
        "meta_batch_size": 1,     # ç›®å‰å¿…é¡»ä¸º 1
        "max_data_size": 20,     # ä¼ ç»™ get_preprocessed_datasets çš„ max_data_size
    }

    # 1) æ„é€  Branin æ•°æ®ï¼ˆcontext pool + å…¨å±€ test ç½‘æ ¼ï¼‰
    X_ctx_pool, X_test_grid, y_ctx_pool, y_test_grid = prepare_data(config)

    # 2) ç”¨æˆ‘ä»¬è‡ªå·±å®šä¹‰çš„ BraninMetaDataset å†æŠ½å‡ ä¸ªä»»åŠ¡çœ‹çœ‹å½¢çŠ¶æ˜¯å¦åˆç†
    demo_meta_dataset(X_ctx_pool, X_test_grid, y_ctx_pool, y_test_grid)

    # 3) åˆå§‹åŒ– TabPFNRegressor
    regressor, regressor_config = setup_regressor(config)

    # 4) æ„å»º fine-tuning dataloaderï¼ˆå¸¦è‡ªå®šä¹‰ Branin splitterï¼‰
    finetuning_dataloader = create_finetuning_dataloader(
        regressor,
        X_ctx_pool,
        y_ctx_pool,
        X_test_grid,
        y_test_grid,
        config,
    )

    # 5) è¿è¡Œå¾®è°ƒ + æ¯ä¸ª epoch ååœ¨å…¨å±€ Branin ç½‘æ ¼ä¸Šè¯„ä¼°
    run_finetuning(
        regressor,
        regressor_config,
        finetuning_dataloader,
        X_ctx_pool,
        y_ctx_pool,
        X_test_grid,
        y_test_grid,
        config,
    )


if __name__ == "__main__":
    main()
