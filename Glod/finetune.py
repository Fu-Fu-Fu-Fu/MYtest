import os
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd
import torch
from torch.optim import Adam
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from scipy.signal import correlate2d
from scipy.ndimage import shift
from scipy.stats import spearmanr

from tabpfn import TabPFNRegressor
from tabpfn.finetune_utils import clone_model_for_evaluation
from tabpfn.utils import meta_dataset_collator
from tabpfn.model_loading import save_tabpfn_model


# ============================================================
# 0. æ ‡å‡† Goldstein-Price å‡½æ•°ï¼ˆnumpy ç‰ˆï¼Œä¸»è¦ç”¨äºæ£€æŸ¥å’Œå¯¹æ¯”ï¼‰
# ============================================================
def goldstein_price_np(x1: np.ndarray, x2: np.ndarray) -> np.ndarray:
    """
    æ ‡å‡† Goldstein-Price å‡½æ•°ï¼Œå®šä¹‰åŸŸé€šå¸¸ä¸º x1, x2 âˆˆ [-2, 2]
    å…¨å±€æœ€å°å€¼åœ¨ (0, -1)ï¼Œå€¼ä¸º 3
    """
    part1 = 1 + (x1 + x2 + 1) ** 2 * (
        19 - 14 * x1 + 3 * x1 ** 2 - 14 * x2 + 6 * x1 * x2 + 3 * x2 ** 2
    )
    part2 = 30 + (2 * x1 - 3 * x2) ** 2 * (
        18 - 32 * x1 + 12 * x1 ** 2 + 48 * x2 - 36 * x1 * x2 + 27 * x2 ** 2
    )
    return (part1 * part2).astype(np.float32)

# ============================================================
# 0.1 ç”Ÿæˆ Goldstein-Price æ•°æ®é›†
# ============================================================
def generate_goldstein_price_dataset(
    n_samples: int = 10_000,
    x1_range: Tuple[float, float] = (-2.0, 2.0),
    x2_range: Tuple[float, float] = (-2.0, 2.0),
    save_path: str = "./data/goldstein_price_dataset.csv",
    seed: int = 42,
) -> None:
    """
    ç”ŸæˆåŸºäº Goldstein-Price å‡½æ•°çš„å¾®è°ƒæ•°æ®é›†ï¼š
    æ¯ä¸€è¡Œæ•°æ®ä¸º x1, x2, yï¼Œå¹¶ä¿å­˜ä¸º CSVã€‚
    """
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    rng = np.random.default_rng(seed=seed)

    # åœ¨ç»™å®šèŒƒå›´å†…å‡åŒ€é‡‡æ · x1, x2
    x1 = rng.uniform(x1_range[0], x1_range[1], size=n_samples).astype(np.float32)
    x2 = rng.uniform(x2_range[0], x2_range[1], size=n_samples).astype(np.float32)

    # è®¡ç®— y = goldstein_price(x1, x2)
    y = goldstein_price_np(x1, x2)

    # ç»„è£…ä¸º DataFrameï¼Œåˆ—é¡ºåºï¼šx1, x2, y
    df = pd.DataFrame({
        "x1": x1,
        "x2": x2,
        "y": y,
    })

    # ä¿å­˜åˆ° CSV æ–‡ä»¶
    df.to_csv(save_path, index=False)
    print(f"âœ… Saved Goldstein-Price dataset with {n_samples} samples to: {save_path}")
    print(f"   x1 range: [{x1.min():.4f}, {x1.max():.4f}]")
    print(f"   x2 range: [{x2.min():.4f}, {x2.max():.4f}]")
    print(f"   y  range: [{y.min():.4f}, {y.max():.4f}]")
    print("---------------------------\n")

# ============================================================
# 1. Goldstein-Price å˜ä½“æ—ï¼ˆtorch ç‰ˆï¼‰ï¼Œç”¨äº similarity ä¸ç”Ÿæˆå˜ä½“å‡½æ•°å€¼
# ============================================================
def goldstein_price_family_torch(
    x: torch.Tensor,
    dx1: float = 0.0,
    dx2: float = 0.0,
    sx1: float = 1.0,
    sx2: float = 1.0,
    alpha: float = 1.0,
    beta: float = 0.0,
) -> torch.Tensor:
    """
    Goldstein-Price å˜ä½“æ—ï¼šåœ¨è¾“å…¥ä¸Šåšçº¿æ€§å˜æ¢ (sx, dx)ï¼Œåœ¨è¾“å‡ºä¸Šåšä»¿å°„å˜æ¢ (alpha, beta)ã€‚
    x: (..., 2) çš„ Tensorï¼Œæœ€åä¸€ç»´ä¸º [x1, x2]
    """
    # æ”¯æŒ (N,2) æˆ– (B,N,2) ç­‰å½¢çŠ¶
    x1 = x[..., 0]
    x2 = x[..., 1]

    # è¾“å…¥å¹³ç§» + ç¼©æ”¾
    x1_t = sx1 * x1 + dx1
    x2_t = sx2 * x2 + dx2

    # Goldstein-Price ä¸»ä½“
    part1 = 1 + (x1_t + x2_t + 1) ** 2 * (
        19 - 14 * x1_t + 3 * x1_t ** 2 - 14 * x2_t + 6 * x1_t * x2_t + 3 * x2_t ** 2
    )
    part2 = 30 + (2 * x1_t - 3 * x2_t) ** 2 * (
        18 - 32 * x1_t + 12 * x1_t ** 2 + 48 * x2_t - 36 * x1_t * x2_t + 27 * x2_t ** 2
    )
    y = part1 * part2

    # è¾“å‡ºä»¿å°„å˜æ¢
    y = alpha * y + beta
    return y


def goldstein_price_family_numpy_from_params(
    X: np.ndarray,
    variant_params: Dict[str, float],
    device: str = "cpu",
) -> np.ndarray:
    """
    æ–¹ä¾¿åœ¨ numpy ä¸Šè°ƒç”¨ goldstein_price_family_torchï¼š
    X: (N, 2) numpy array
    è¿”å›: (N,) numpy array
    """
    x_tensor = torch.from_numpy(X).float().unsqueeze(0).to(device)  # (1, N, 2)
    with torch.no_grad():
        y = goldstein_price_family_torch(x_tensor, **variant_params).cpu().numpy()[0]
    return y.astype(np.float32)


# ============================================================
# 2. ç›¸ä¼¼åº¦åº¦é‡ï¼šå¯¹é½ Spearmanï¼ˆä¸ Branin ç‰ˆæœ¬ç›¸åŒï¼‰
# ============================================================
def find_best_alignment_1d(
    y_source: np.ndarray,
    y_target: np.ndarray,
    grid_shape: Tuple[int, int],
) -> Tuple[float, np.ndarray]:
    """
    ä½¿ç”¨ 2D äº’ç›¸å…³æ‰¾åˆ°æœ€ä½³å¯¹é½ä½ç½®ï¼ˆ1D æ•°æ®éœ€è½¬å› 2D ç½‘æ ¼ï¼‰
    è¿”å›: (aligned_spearman, shift_vec)
    """
    # å°† 1D æ•°ç»„è½¬ä¸º 2D ç½‘æ ¼
    Z_source = y_source.reshape(grid_shape)
    Z_target = y_target.reshape(grid_shape)

    # æ ‡å‡†åŒ–
    Z_source_norm = (Z_source - np.mean(Z_source)) / (np.std(Z_source) + 1e-8)
    Z_target_norm = (Z_target - np.mean(Z_target)) / (np.std(Z_target) + 1e-8)

    # 2D äº’ç›¸å…³
    correlation = correlate2d(Z_source_norm, Z_target_norm, mode="same")

    # æ‰¾åˆ°æœ€å¤§ç›¸å…³ä½ç½®
    max_idx = np.unravel_index(np.argmax(correlation), correlation.shape)
    center = np.array(correlation.shape) // 2
    shift_vec = np.array(max_idx) - center

    # å¯¹ç›®æ ‡å‡½æ•°è¿›è¡Œä½ç§»
    Z_target_shifted = shift(Z_target, shift_vec, mode="constant", cval=np.nan)

    # åªåœ¨æœ‰æ•ˆåŒºåŸŸè®¡ç®— Spearmanï¼ˆæ’é™¤ NaNï¼‰
    valid_mask = ~np.isnan(Z_target_shifted)

    total_points = Z_source.size
    overlap_points = np.sum(valid_mask)
    overlap_ratio = overlap_points / total_points

    if np.sum(valid_mask) < 10:
        # æœ‰æ•ˆç‚¹å¤ªå°‘å°±æ”¾å¼ƒ
        return 0.0, shift_vec

    source_flat = Z_source[valid_mask].flatten()
    target_flat = Z_target_shifted[valid_mask].flatten()

    # è®¡ç®—å¯¹é½åçš„ Spearman
    rho, _ = spearmanr(source_flat, target_flat)
    if rho is None:
        rho = 0.0
    rho = float(rho) * float(overlap_ratio)  # ä¹˜ä»¥é‡å ç‡ä½œä¸ºæƒ©ç½š

    return rho, shift_vec


def calculate_similarity(
    variant_params: Dict[str, float],
    grid_points: np.ndarray,
    y_base: np.ndarray,
    grid_shape: Tuple[int, int],
    device: str = "cpu",
) -> float:
    """
    è®¡ç®—å˜ä½“ä¸æ ‡å‡† Goldstein-Price åœ¨åŒä¸€ç½‘æ ¼ä¸Šçš„å¯¹é½ Spearman ç›¸å…³ç³»æ•°ã€‚
    """
    x_tensor = torch.tensor(grid_points, dtype=torch.float32, device=device).unsqueeze(0)
    with torch.no_grad():
        y_var = (
            goldstein_price_family_torch(x_tensor, **variant_params)
            .cpu()
            .numpy()
            .flatten()
            .astype(np.float32)
        )

    rho_aligned, _ = find_best_alignment_1d(y_base, y_var, grid_shape)
    return float(rho_aligned)


# ============================================================
# 3. ç”ŸæˆåŸºç¡€ç½‘æ ¼ & é€‰æ‹© 32 ä¸ªå˜ä½“
# ============================================================
def build_base_grid_and_y(config: dict):
    """
    æ„é€ æ ‡å‡† Goldstein-Price çš„å…¨å±€æµ‹è¯•ç½‘æ ¼ï¼š
      - X_test_grid: (G^2, 2)
      - X1, X2: (G, G) meshgrid
      - y_base: (G^2,) æ ‡å‡† Goldstein-Price åœ¨è¯¥ç½‘æ ¼ä¸Šçš„å€¼
    """
    x1_min, x1_max = config.get("x1_range", (-2.0, 2.0))
    x2_min, x2_max = config.get("x2_range", (-2.0, 2.0))
    grid_size = config.get("grid_size", 20)

    x1_lin = np.linspace(x1_min, x1_max, grid_size, dtype=np.float32)
    x2_lin = np.linspace(x2_min, x2_max, grid_size, dtype=np.float32)
    X1, X2 = np.meshgrid(x1_lin, x2_lin)

    X_test_grid = np.stack([X1.ravel(), X2.ravel()], axis=1).astype(np.float32)
    y_base = goldstein_price_np(X_test_grid[:, 0], X_test_grid[:, 1])  # (G^2,)

    print(f"Base grid: {X_test_grid.shape[0]} points, grid_size={grid_size}")
    print(f"  X_test_grid shape: {X_test_grid.shape}")
    print(f"  y_base shape      : {y_base.shape}")
    print("---------------------------\n")

    return X_test_grid, X1, X2, y_base


def sample_variant_params(rng: np.random.Generator) -> Dict[str, float]:
    """
    éšæœºé‡‡æ ·ä¸€ç»„ Goldstein-Price å˜ä½“å‚æ•°ã€‚
    ç”±äº Goldstein-Price å®šä¹‰åŸŸè¾ƒå° [-2, 2]ï¼Œéœ€è¦è°ƒæ•´æ‰°åŠ¨èŒƒå›´ã€‚
    """
    params = {
        # è¾“å…¥å¹³ç§»ï¼ˆèŒƒå›´è¾ƒå°ï¼Œå› ä¸ºå®šä¹‰åŸŸåªæœ‰ [-2, 2]ï¼‰
        "dx1": float(rng.uniform(-0.5, 0.5)),
        "dx2": float(rng.uniform(-0.5, 0.5)),
        # è¾“å…¥ç¼©æ”¾
        "sx1": float(rng.uniform(0.8, 1.2)),
        "sx2": float(rng.uniform(0.8, 1.2)),
        # è¾“å‡ºç¼©æ”¾ä¸å¹³ç§»ï¼ˆGoldstein-Price å€¼åŸŸå¾ˆå¤§ï¼Œéœ€è¦ç›¸åº”è°ƒæ•´ï¼‰
        "alpha": float(rng.uniform(0.8, 1.2)),
        "beta": float(rng.uniform(-1000.0, 1000.0)),
    }
    return params


def select_goldstein_price_variants(
    config: dict,
    grid_points: np.ndarray,
    y_base: np.ndarray,
    grid_shape: Tuple[int, int],
) -> Tuple[List[Dict[str, float]], np.ndarray, List[float]]:
    """
    ç”¨æ‹’ç»é‡‡æ ·çš„æ–¹å¼ï¼Œé€‰å‡ºè‹¥å¹²ä¸ªç›¸ä¼¼åº¦åœ¨ [rho_min, rho_max] çš„ Goldstein-Price å˜ä½“ã€‚
    è¿”å›ï¼š
      - variants:       é•¿åº¦ num_variants çš„å‚æ•°å­—å…¸åˆ—è¡¨
      - variant_y_grids: shape (num_variants, G^2)ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªå˜ä½“åœ¨å…¨å±€ç½‘æ ¼ä¸Šçš„å€¼
      - variant_rhos:   æ¯ä¸ªå˜ä½“å¯¹åº”çš„ç›¸ä¼¼åº¦ rho
    """
    device = config["device"]
    rng = np.random.default_rng(config.get("variant_seed", config["random_seed"]))

    num_variants = config.get("num_variants", 32)
    rho_min, rho_max = config.get("similarity_range", (0.4, 0.6))
    max_trials = config.get("max_variant_trials", num_variants * 200)

    variants: List[Dict[str, float]] = []
    variant_y_list: List[np.ndarray] = []
    variant_rhos: List[float] = []

    print(
        f"--- Select Goldstein-Price variants ---\n"
        f"Target similarity range: [{rho_min:.2f}, {rho_max:.2f}], "
        f"num_variants={num_variants}, max_trials={max_trials}"
    )

    trials = 0
    while len(variants) < num_variants and trials < max_trials:
        trials += 1
        params = sample_variant_params(rng)
        rho = calculate_similarity(params, grid_points, y_base, grid_shape, device=device)

        if rho_min <= rho <= rho_max:
            # è®¡ç®—å¹¶ç¼“å­˜å˜ä½“åœ¨ç½‘æ ¼ä¸Šçš„å€¼
            x_tensor = torch.tensor(grid_points, dtype=torch.float32, device=device).unsqueeze(0)
            with torch.no_grad():
                y_var = (
                    goldstein_price_family_torch(x_tensor, **params)
                    .cpu()
                    .numpy()
                    .flatten()
                    .astype(np.float32)
                )

            variants.append(params)
            variant_y_list.append(y_var)
            variant_rhos.append(rho)

            print(
                f"  Accepted variant {len(variants):2d}/{num_variants} "
                f"(trial={trials:4d}), rho={rho:.3f}"
            )

    if len(variants) < num_variants:
        print(
            f"WARNING: Only found {len(variants)} variants in range [{rho_min},{rho_max}] "
            f"after {trials} trials."
        )

    variant_y_grids = np.stack(variant_y_list, axis=0)  # (num_variants, G^2)

    print("\nSelected variants summary:")
    for i, rho in enumerate(variant_rhos):
        print(f"  Variant {i:2d}: rho={rho:.3f}")
    print("---------------------------\n")

    return variants, variant_y_grids, variant_rhos


# ============================================================
# 4. ä¸ºæ¯ä¸ªå˜ä½“æ„é€ ä¸Šä¸‹æ–‡æ± ï¼ˆåŸºäºå·²æœ‰ goldstein_price_dataset.csv çš„ Xï¼‰
# ============================================================
def load_base_context_points(config: dict) -> Tuple[np.ndarray, np.ndarray]:
    """
    ä» goldstein_price_dataset.csv ä¸­è¯»å–åŸºç¡€ä¸Šä¸‹æ–‡ç‚¹ï¼š
      - X_ctx_base: (N_ctx_base, 2)
      - y_ctx_base: (N_ctx_base,) â€”â€” æ ‡å‡† Goldstein-Price åœ¨è¿™äº›ç‚¹ä¸Šçš„å€¼
    """
    data_path = config.get("data_path", "./data/goldstein_price_dataset.csv")
    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™ç”Ÿæˆ
    if not os.path.exists(data_path):
        print(f"âš ï¸ Dataset not found at {data_path}, generating...")
        generate_goldstein_price_dataset(
            n_samples=config.get("num_context_samples", 10_000),
            x1_range=config.get("x1_range", (-2.0, 2.0)),
            x2_range=config.get("x2_range", (-2.0, 2.0)),
            save_path=data_path,
            seed=config.get("random_seed", 42),
        )

    df = pd.read_csv(data_path)

    X_all = df[["x1", "x2"]].values.astype(np.float32)
    y_all = df["y"].values.astype(np.float32)

    rng = np.random.default_rng(config.get("random_seed", 42))
    num_use = min(config.get("num_context_samples", len(y_all)), len(y_all))
    idx = rng.choice(len(y_all), size=num_use, replace=False)

    X_ctx_base = X_all[idx]
    y_ctx_base = y_all[idx]

    print("--- Base context pool from CSV ---")
    print(f"Base context pool size: {X_ctx_base.shape[0]}")
    print(f"  X_ctx_base shape: {X_ctx_base.shape}")
    print(f"  y_ctx_base shape: {y_ctx_base.shape}")
    print("---------------------------\n")

    return X_ctx_base, y_ctx_base


def build_variant_context_pools(
    X_ctx_base: np.ndarray,
    variants: List[Dict[str, float]],
    config: dict,
) -> Tuple[List[np.ndarray], List[np.ndarray]]:
    """
    å¯¹æ¯ä¸ªå˜ä½“ iï¼š
      - X_ctx_pools[i] = X_ctx_baseï¼ˆæ‰€æœ‰å˜ä½“å…±ç”¨åŒä¸€æ‰¹åæ ‡ï¼‰
      - y_ctx_pools[i] = è¯¥å˜ä½“åœ¨ X_ctx_base ä¸Šçš„å‡½æ•°å€¼
    """
    device = config["device"]
    X_ctx_pools: List[np.ndarray] = []
    y_ctx_pools: List[np.ndarray] = []

    print("--- Build variant context pools ---")
    for i, params in enumerate(variants):
        y_ctx = goldstein_price_family_numpy_from_params(X_ctx_base, params, device=device)
        X_ctx_pools.append(X_ctx_base.copy())
        y_ctx_pools.append(y_ctx)

        if i < 3:  # æ‰“å°å‰å‡ ä¸ªå˜ä½“çš„å‰å‡ æ¡æ ·æœ¬çœ‹çœ‹
            print(f"\nVariant {i} context sample (first 5 points):")
            print(
                np.concatenate(
                    [X_ctx_base[:5], y_ctx[:5, None]], axis=1
                )
            )

    print("\nTotal variant context pools:", len(X_ctx_pools))
    print("---------------------------\n")

    return X_ctx_pools, y_ctx_pools


# ============================================================
# 5. å¤šå˜ä½“ meta-datasetï¼ˆè·¯å¾„ Bï¼‰â€”â€” ç”¨äºç›´è§‚æŸ¥çœ‹ task ç»“æ„
# ============================================================
class GoldsteinPriceFamilyMetaDataset(Dataset):
    """
    æ¯ä¸ª __getitem__ è¿”å›ä¸€ä¸ª"æºä»»åŠ¡"ï¼ˆmeta-taskï¼‰ï¼š
      - X_ctx:  (n_ctx, 2)   â€”â€” æ¥è‡ªæŸä¸€ä¸ªå˜ä½“çš„ä¸Šä¸‹æ–‡ç‚¹
      - y_ctx:  (n_ctx,)
      - X_test: (n_test, 2)  â€”â€” å…¬å…±å…¨å±€ç½‘æ ¼
      - y_test: (n_test,)    â€”â€” å¯¹åº”å˜ä½“åœ¨è¯¥ç½‘æ ¼ä¸Šçš„å‡½æ•°å€¼
    """

    def __init__(
        self,
        X_ctx_pools: List[np.ndarray],
        y_ctx_pools: List[np.ndarray],
        X_test_grid: np.ndarray,
        y_test_grids: np.ndarray,  # shape (num_variants, n_test)
        num_tasks: int,
        min_ctx: int,
        max_ctx: int,
        random_seed: int = 42,
    ):
        assert len(X_ctx_pools) == len(y_ctx_pools) == y_test_grids.shape[0]
        assert min_ctx >= 1 and max_ctx >= min_ctx

        self.X_ctx_pools = X_ctx_pools
        self.y_ctx_pools = y_ctx_pools
        self.X_test_grid = X_test_grid
        self.y_test_grids = y_test_grids

        self.num_variants = len(X_ctx_pools)
        self.num_tasks = num_tasks
        self.min_ctx = min_ctx
        self.max_ctx = max_ctx

        self.rng = np.random.default_rng(random_seed)

    def __len__(self) -> int:
        return self.num_tasks

    def __getitem__(self, idx: int):
        # 1) éšæœºé€‰æ‹©ä¸€ä¸ªå˜ä½“
        v = int(self.rng.integers(0, self.num_variants))

        X_pool = self.X_ctx_pools[v]
        y_pool = self.y_ctx_pools[v]
        n_pool = X_pool.shape[0]

        # 2) éšæœºé€‰æ‹©ä¸Šä¸‹æ–‡é•¿åº¦ n_ctx âˆˆ [min_ctx, max_ctx]
        n_ctx = int(self.rng.integers(self.min_ctx, self.max_ctx + 1))

        # 3) åœ¨è¯¥å˜ä½“çš„ä¸Šä¸‹æ–‡æ± ä¸­é‡‡æ · n_ctx ä¸ªç‚¹
        indices = self.rng.choice(n_pool, size=n_ctx, replace=False)
        X_ctx = X_pool[indices]
        y_ctx = y_pool[indices]

        # 4) test ä½¿ç”¨å…¬å…±ç½‘æ ¼ + è¯¥å˜ä½“åœ¨ç½‘æ ¼ä¸Šçš„å‡½æ•°å€¼
        X_test = self.X_test_grid
        y_test = self.y_test_grids[v]

        return (
            X_ctx.astype(np.float32),
            y_ctx.astype(np.float32),
            X_test.astype(np.float32),
            y_test.astype(np.float32),
        )


def demo_family_meta_dataset(
    X_ctx_pools: List[np.ndarray],
    y_ctx_pools: List[np.ndarray],
    X_test_grid: np.ndarray,
    y_test_grids: np.ndarray,
    config: dict,
):
    """
    æ‰“å°å‡ ä¸ª meta-task çœ‹çœ‹ç»“æ„æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚
    """
    num_tasks_demo = 5
    ds = GoldsteinPriceFamilyMetaDataset(
        X_ctx_pools=X_ctx_pools,
        y_ctx_pools=y_ctx_pools,
        X_test_grid=X_test_grid,
        y_test_grids=y_test_grids,
        num_tasks=num_tasks_demo,
        min_ctx=config["min_context"],
        max_ctx=config["max_context"],
        random_seed=config["random_seed"],
    )

    loader = DataLoader(ds, batch_size=1, shuffle=False)

    print("--- Demo GoldsteinPriceFamilyMetaDataset tasks ---")
    for batch_idx, batch in enumerate(loader):
        X_ctx, y_ctx, X_test, y_test = batch

        # batch_size = 1ï¼Œå»æ‰ batch ç»´åº¦æ–¹ä¾¿æ‰“å°
        X_ctx = X_ctx[0].numpy()
        y_ctx = y_ctx[0].numpy()
        X_test = X_test[0].numpy()
        y_test = y_test[0].numpy()

        print(f"\nTask {batch_idx}:")
        print(f"  X_ctx shape:  {X_ctx.shape}")
        print(f"  y_ctx shape:  {y_ctx.shape}")
        print(f"  X_test shape: {X_test.shape}")
        print(f"  y_test shape: {y_test.shape}")

        print("  Sample context points (x1, x2, y):")
        print(np.concatenate([X_ctx[:3], y_ctx[:3, None]], axis=1))

        print("  Sample test grid points (x1, x2, y):")
        print(np.concatenate([X_test[:3], y_test[:3, None]], axis=1))

    print("---------------------------\n")


# ============================================================
# 6. TabPFNRegressor åˆå§‹åŒ– & å¤šå˜ä½“ splitter + dataloader
# ============================================================
def setup_regressor(config: dict) -> Tuple[TabPFNRegressor, dict]:
    print("--- TabPFNRegressor Setup ---")
    regressor_config = {
        "ignore_pretraining_limits": True,
        "device": config["device"],
        "n_estimators": 1,
        "random_state": config["random_seed"],
        "inference_precision": torch.float32,
    }
    regressor = TabPFNRegressor(
        **regressor_config,
        fit_mode="batched",
        differentiable_input=False,
    )
    print(f"Using device: {config['device']}")
    print("---------------------------\n")
    return regressor, regressor_config


def make_goldstein_price_family_splitter(
    X_ctx_pools: List[np.ndarray],
    y_ctx_pools: List[np.ndarray],
    X_test_grid: np.ndarray,
    y_test_grids: np.ndarray,
    config: dict,
):
    """
    è‡ªå®šä¹‰ splitterï¼Œç”¨äº TabPFNRegressor.get_preprocessed_datasetsã€‚
    """
    num_variants = len(X_ctx_pools)
    min_c = config["min_context"]
    max_c = config["max_context"]
    rng = np.random.default_rng(config["random_seed"])

    def splitter(X_all: np.ndarray, y_all: np.ndarray):
        # 1) éšæœºé€‰æ‹©å˜ä½“
        v = int(rng.integers(0, num_variants))

        X_pool = X_ctx_pools[v]
        y_pool = y_ctx_pools[v]
        n_pool = X_pool.shape[0]

        # 2) éšæœºä¸Šä¸‹æ–‡é•¿åº¦
        ctx_size = int(rng.integers(min_c, max_c + 1))
        indices = rng.choice(n_pool, size=ctx_size, replace=False)

        X_ctx = X_pool[indices]
        y_ctx = y_pool[indices]

        # 3) test å›ºå®šä¸ºå…¬å…±ç½‘æ ¼ + è¯¥å˜ä½“å¯¹åº”çš„ y
        X_test = X_test_grid
        y_test = y_test_grids[v]

        return X_ctx, X_test, y_ctx, y_test

    return splitter


def create_finetuning_dataloader_family(
    regressor: TabPFNRegressor,
    X_ctx_base: np.ndarray,
    y_ctx_base: np.ndarray,
    X_ctx_pools: List[np.ndarray],
    y_ctx_pools: List[np.ndarray],
    X_test_grid: np.ndarray,
    y_test_grids: np.ndarray,
    config: dict,
) -> DataLoader:
    print("--- Build finetuning datasets & dataloader (family) ---")

    splitter = make_goldstein_price_family_splitter(
        X_ctx_pools,
        y_ctx_pools,
        X_test_grid,
        y_test_grids,
        config,
    )

    max_data_size = config["finetuning"]["max_data_size"]

    training_datasets = regressor.get_preprocessed_datasets(
        X_ctx_base,
        y_ctx_base,
        splitter,
        max_data_size=max_data_size,
    )

    print(
        f"Number of meta-datasets from get_preprocessed_datasets: "
        f"{len(training_datasets)}"
    )

    finetuning_dataloader = DataLoader(
        training_datasets,
        batch_size=config["finetuning"]["meta_batch_size"],
        collate_fn=meta_dataset_collator,
    )

    # çœ‹ä¸€ä¸ª batch ç¡®è®¤å½¢çŠ¶
    first_batch = next(iter(finetuning_dataloader))
    (
        X_trains_preprocessed,
        X_tests_preprocessed,
        y_trains_znorm,
        y_test_znorm,
        cat_ixs,
        confs,
        raw_space_bardist_,
        znorm_space_bardist_,
        _,
        _y_test_raw,
    ) = first_batch

    print(
        "Inspect one preprocessed task (after collate, meta_batch_size=1):"
    )
    print("  X_trains_preprocessed[0].shape:", X_trains_preprocessed[0].shape)
    print("  X_tests_preprocessed[0].shape :", X_tests_preprocessed[0].shape)
    print("  y_trains_znorm[0].shape       :", y_trains_znorm[0].shape)
    print("  y_test_znorm[0].shape         :", y_test_znorm[0].shape)
    print("---------------------------\n")

    return finetuning_dataloader


# ============================================================
# 7. åœ¨æ ‡å‡† Goldstein-Price ç½‘æ ¼ä¸Šçš„ç®€å•è¯„ä¼°ï¼ˆç”¨äºè®­ç»ƒè¿‡ç¨‹ç›‘æ§ï¼‰
# ============================================================
def evaluate_regressor_on_base_goldstein_price_grid(
    regressor: TabPFNRegressor,
    regressor_config: dict,
    X_ctx_base: np.ndarray,
    y_ctx_base: np.ndarray,
    X_test_grid: np.ndarray,
    y_base_grid: np.ndarray,
    config: dict,
) -> Tuple[float, float, float]:
    """
    åœ¨æ ‡å‡† Goldstein-Price çš„å…¨å±€ç½‘æ ¼ä¸Šè¯„ä¼°å½“å‰ TabPFNã€‚
    """
    eval_regressor = clone_model_for_evaluation(
        regressor,
        {
            **regressor_config,
            "inference_config": {
                "SUBSAMPLE_SAMPLES": config["n_inference_context_samples"],
            },
        },
        TabPFNRegressor,
    )

    rng = np.random.default_rng(config["random_seed"])
    n_ctx_eval = config.get("eval_context_size", config["max_context"])
    n_base = X_ctx_base.shape[0]
    idx = rng.choice(np.arange(n_base), size=n_ctx_eval, replace=False)
    X_ctx_eval = X_ctx_base[idx]
    y_ctx_eval = y_ctx_base[idx]

    eval_regressor.fit(X_ctx_eval, y_ctx_eval)
    preds = eval_regressor.predict(X_test_grid)

    mse = mean_squared_error(y_base_grid, preds)
    mae = mean_absolute_error(y_base_grid, preds)
    r2 = r2_score(y_base_grid, preds)
    return mse, mae, r2


# ============================================================
# 8. å¾®è°ƒä¸»å¾ªç¯ï¼ˆåœ¨ Goldstein-Price å˜ä½“æ—ä¸Š finetune TabPFNï¼‰
# ============================================================
def run_finetuning_family(
    regressor: TabPFNRegressor,
    regressor_config: dict,
    finetuning_dataloader: DataLoader,
    X_ctx_base: np.ndarray,
    y_ctx_base: np.ndarray,
    X_test_grid: np.ndarray,
    y_base_grid: np.ndarray,
    config: dict,
) -> None:
    # ç¡®ä¿åªæœ‰ä¸€ä¸ªåº•å±‚æ¨¡å‹å¯å¾®è°ƒ
    if hasattr(regressor, "models_"):
        if len(regressor.models_) > 1:
            raise ValueError(
                f"Your TabPFNRegressor uses multiple models ({len(regressor.models_)}). "
                "Finetuning is only supported for a single model."
            )
        model = regressor.models_[0]
    else:
        model = regressor.model_

    optimizer = Adam(model.parameters(), lr=config["finetuning"]["learning_rate"])
    print(
        f"--- Optimizer Initialized: Adam, LR: {config['finetuning']['learning_rate']} ---\n"
    )

    print("--- Start finetuning on Goldstein-Price family ---")
    num_epochs = config["finetuning"]["epochs"]

    for epoch in range(num_epochs + 1):
        # 1) æ¯ä¸ª epoch å‰å…ˆåœ¨æ ‡å‡† Goldstein-Price ç½‘æ ¼ä¸Šè¯„ä¼°ä¸€æ¬¡
        mse, mae, r2 = evaluate_regressor_on_base_goldstein_price_grid(
            regressor,
            regressor_config,
            X_ctx_base,
            y_ctx_base,
            X_test_grid,
            y_base_grid,
            config,
        )
        status = "Initial" if epoch == 0 else f"Epoch {epoch}"
        print(
            f"ğŸ“Š {status} Evaluation on base Goldstein-Price grid | "
            f"MSE: {mse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}"
        )

        if epoch == 0:
            print("---------------------------")
            continue

        # 2) åœ¨å˜ä½“æ—æ„æˆçš„ meta-tasks ä¸Šåšä¸€æ¬¡ epoch çš„å¾®è°ƒ
        progress_bar = tqdm(
            finetuning_dataloader, desc=f"Finetuning Epoch {epoch}"
        )
        for data_batch in progress_bar:
            optimizer.zero_grad()

            (
                X_trains_preprocessed,
                X_tests_preprocessed,
                y_trains_znorm,
                y_test_znorm,
                cat_ixs,
                confs,
                raw_space_bardist_,
                znorm_space_bardist_,
                _,
                _y_test_raw,
            ) = data_batch

            regressor.raw_space_bardist_ = raw_space_bardist_[0]
            regressor.znorm_space_bardist_ = znorm_space_bardist_[0]

            regressor.fit_from_preprocessed(
                X_trains_preprocessed,
                y_trains_znorm,
                cat_ixs,
                confs,
            )

            logits, _, _ = regressor.forward(X_tests_preprocessed)

            loss_fn = znorm_space_bardist_[0]
            y_target = y_test_znorm

            loss = loss_fn(logits, y_target.to(config["device"])).mean()
            loss.backward()
            optimizer.step()

            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

        print("---------------------------")

    # è®­ç»ƒç»“æŸåï¼Œä¿å­˜æ•´ä¸ª TabPFN æ¨¡å‹
    os.makedirs("./model", exist_ok=True)
    save_path = "./model/finetuned_tabpfn_goldstein_price_family.ckpt"
    save_tabpfn_model(regressor, save_path)
    print(f"Saved fine-tuned TabPFNRegressor to: {save_path}")
    print("--- âœ… Finetuning Finished ---")


# ============================================================
# 9. mainï¼šä¸²èµ·æ¥æ­¥éª¤ 0â€“5
# ============================================================
def main():
    # å…¨å±€é…ç½®
    config = {
        "device": "cuda" if torch.cuda.is_available() else "cpu",
        "random_seed": 42,
        "variant_seed": 123,

        # Goldstein-Price åŸŸ & ç½‘æ ¼ï¼ˆå®šä¹‰åŸŸä¸º [-2, 2] x [-2, 2]ï¼‰
        "x1_range": (-2.0, 2.0),
        "x2_range": (-2.0, 2.0),
        "grid_size": 20,  # 20Ã—20 ç½‘æ ¼

        # ä» CSV è¯»å–çš„åŸºç¡€ä¸Šä¸‹æ–‡æ± å¤§å°ï¼ˆæ ‡å‡† Goldstein-Priceï¼‰
        "data_path": "./data/goldstein_price_dataset.csv",
        "num_context_samples": 10_000,

        # å˜ä½“ç›¸å…³
        "num_variants": 32,
        "similarity_range": (0.4, 0.6),
        "max_variant_trials": 32 * 300,

        # å˜é•¿ä¸Šä¸‹æ–‡
        "min_context": 2,
        "max_context": 20,

        # è¯„ä¼°æ—¶ç”¨å¤šå°‘ä¸Šä¸‹æ–‡ç‚¹
        "eval_context_size": 20,
        "n_inference_context_samples": 20,

        # å¾®è°ƒè¶…å‚æ•°
        "finetuning": {
            "epochs": 10,
            "learning_rate": 1.5e-6,
            "meta_batch_size": 1,
            "max_data_size": 20,
        },
    }

    # --- æ­¥éª¤ 0ï¼šç”Ÿæˆæ•°æ®é›†ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ ---
    if not os.path.exists(config["data_path"]):
        print("=" * 50)
        print("Step 0: Generating Goldstein-Price dataset...")
        print("=" * 50)
        generate_goldstein_price_dataset(
            n_samples=config["num_context_samples"],
            x1_range=config["x1_range"],
            x2_range=config["x2_range"],
            save_path=config["data_path"],
            seed=config["random_seed"],
        )

    # --- æ­¥éª¤ 1ï¼šæ„é€ åŸºç¡€ç½‘æ ¼ & æ ‡å‡† Goldstein-Price ç½‘æ ¼å€¼ ---
    X_test_grid, X1, X2, y_base_grid = build_base_grid_and_y(config)

    # --- æ­¥éª¤ 2ï¼šä» CSV è¯»å–åŸºç¡€ä¸Šä¸‹æ–‡ç‚¹ï¼ˆæ ‡å‡† Goldstein-Priceï¼‰ ---
    X_ctx_base, y_ctx_base = load_base_context_points(config)

    # --- æ­¥éª¤ 3ï¼šåœ¨åŸºç¡€ç½‘æ ¼ä¸Šé€‰ 32 ä¸ªç›¸ä¼¼åº¦åœ¨ [0.4,0.6] çš„å˜ä½“ ---
    grid_shape = (config["grid_size"], config["grid_size"])
    variants, variant_y_grids, variant_rhos = select_goldstein_price_variants(
        config,
        X_test_grid,
        y_base_grid,
        grid_shape,
    )

    os.makedirs("./data", exist_ok=True)
    np.savez(
        "./data/goldstein_price_family_variants.npz",
        variants=np.array(variants, dtype=object),
        variant_y_grids=variant_y_grids,
        variant_rhos=np.array(variant_rhos, dtype=np.float32),
    )
    print("Saved Goldstein-Price family variants to ./data/goldstein_price_family_variants.npz")

    # --- æ­¥éª¤ 4ï¼šä¸ºæ¯ä¸ªå˜ä½“æ„é€ ä¸Šä¸‹æ–‡æ±  ---
    X_ctx_pools, y_ctx_pools = build_variant_context_pools(
        X_ctx_base,
        variants,
        config,
    )

    # --- æ­¥éª¤ 5ï¼ˆè·¯å¾„ Bï¼‰ï¼šæ„é€  GoldsteinPriceFamilyMetaDatasetï¼Œæ‰“å°å‡ ä¸ª task çœ‹çœ‹ ---
    demo_family_meta_dataset(
        X_ctx_pools,
        y_ctx_pools,
        X_test_grid,
        variant_y_grids,
        config,
    )

    # --- æ­¥éª¤ 6ï¼šTabPFN åˆå§‹åŒ– + finetuning dataloader + å¾®è°ƒ ---
    regressor, regressor_config = setup_regressor(config)

    finetuning_dataloader = create_finetuning_dataloader_family(
        regressor,
        X_ctx_base,
        y_ctx_base,
        X_ctx_pools,
        y_ctx_pools,
        X_test_grid,
        variant_y_grids,
        config,
    )

    run_finetuning_family(
        regressor,
        regressor_config,
        finetuning_dataloader,
        X_ctx_base,
        y_ctx_base,
        X_test_grid,
        y_base_grid,
        config,
    )


if __name__ == "__main__":
    main()